{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative machine learning - Advanced neural networks\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)\n",
    "\n",
    "In this course we will cover\n",
    "1. An introduction to [convolutions](#convolution) and how they can be used in CNN\n",
    "2. Defining a [convolutional Neural Network](#cnn) in Pytorch for image classification \n",
    "3. Coding our own [convolutional layer](#layer)\n",
    "4. An explanation on [recurrent networks](#rnn) in practice\n",
    "5. An exercise on [symbolic music generation](#music) with LSTM in both Pytorch and JAX\n",
    "\n",
    "As usual, we are going to use relatively _low-level_ libraries to perform the first exercises (implementing convolutions). To observe this idea in simple setups, we are going to use the `numpy` library and also initialize the homemade course library `cml` and style for future plotting and exercise. We also set the random generator to a fixed point with `rng = np.random.RandomState(1)`, to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\ANACONDA\\envs\\ml_win\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\ANACONDA\\envs\\ml_win\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\ANACONDA\\envs\\ml_win\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"ba5ae594-904d-475b-8d56-82d03c66f7ff\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"ba5ae594-904d-475b-8d56-82d03c66f7ff\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\", \"https://unpkg.com/@holoviz/panel@1.4.1/dist/panel.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"ba5ae594-904d-475b-8d56-82d03c66f7ff\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'ace': '//cdnjs.cloudflare.com/ajax/libs/ace/1.4.7'}, 'shim': {'ace/ext-language_tools': {'deps': ['ace/ace']}, 'ace/ext-modelist': {'deps': ['ace/ace']}}});\n      require([\"ace/ace\"], function(ace) {\n\twindow.ace = ace\n\ton_load()\n      })\n      require([\"ace/ext-language_tools\"], function() {\n\ton_load()\n      })\n      require([\"ace/ext-modelist\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 3;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.ace !== undefined) && (!(window.ace instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js', 'https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ace.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-language_tools.js\", \"https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.11/ext-modelist.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"/*\\n ~ CML // Creative Machine Learning ~\\n mml.css : CSS styling information for Panel and Bokeh\\n \\n This file defines the main CSS styling information for the CML course\\n \\n Author               :  Philippe Esling\\n                        <esling@ircam.fr>\\n*/\\n\\nbody {\\n  display: flex;\\n  height: 100vh;\\n  margin: 0px;\\n  overflow-x: hidden;\\n  overflow-y: hidden;\\n}\\n\\n.bk-root .bk, .bk-root .bk:before, .bk-root .bk:after {\\n  font-family: \\\"Josefin Sans\\\";\\n}\\n\\nimg {\\n  max-width: 100%;\\n}\\n\\n#container {\\n  padding:0px;\\n  height:100vh;\\n  width: 100vw;\\n  max-width: 100vw;\\n}\\n\\n#sidebar .mdc-list {\\n  padding-left: 5px;\\n  padding-right: 5px;\\n}\\n\\n.mdc-drawer-app-content {\\n  flex: auto;\\n  position: relative;\\n  overflow: hidden;\\n}\\n\\n.mdc-drawer {\\n  background: #FAFAFA; /* GRAY 50 */\\n}\\n\\n.mdc-drawer-app-content {\\n  margin-left: 0 !important;\\n}\\n\\n.title-bar {\\n  display: contents;\\n  justify-content: center;\\n  align-content: center;\\n  width: 100%;\\n}\\n\\n.mdc-top-app-bar .bk-menu {\\n  color: black\\n}\\n\\n.app-header {\\n  display: contents;\\n  padding-left: 10px;\\n  font-size: 1.25em;\\n}\\n\\nimg.app-logo {\\n  padding-right: 10px;\\n  font-size: 28px;\\n  height: 30px;\\n  max-width: inherit;\\n  padding-top: 12px;\\n  padding-bottom: 6px;\\n}\\n\\n#app-title {\\n  padding-right: 12px;\\n  padding-left: 12px;\\n}\\n\\n.title {\\n  font-family: \\\"Josefin Sans\\\";\\n  color: #fff;\\n  text-decoration: none;\\n  text-decoration-line: none;\\n  text-decoration-style: initial;\\n  text-decoration-color: initial;\\n  font-weight: 400;\\n  font-size: 2em;\\n  line-height: 2em;\\n  white-space: nowrap;\\n}\\n\\n.main-content {\\n  overflow-y: scroll;\\n  overflow-x: auto;\\n}\\n\\n#header {\\n  position: absolute;\\n  z-index: 7;\\n}\\n\\n#header-items {\\n  width: 100%;\\n  margin-left:15px;\\n}\\n\\n.pn-busy-container {\\n  align-items: center;\\n  justify-content: center;\\n  display: flex;\\n}\\n\\n.mdc-drawer__content {\\n  overflow-x: hidden;\\n}\\n.mdc-drawer__content, .main-content {\\n  padding: 12px;\\n}\\n\\n.main-content {\\n  height: calc(100vh - 88px);\\n  max-height: calc(100vh - 88px);\\n  padding-right: 32px;\\n}\\n\\nbutton.mdc-button.mdc-card-button {\\n  color: transparent;\\n  height: 50px;\\n}\\n\\np.mdc-button {\\n  display: none;\\n}\\n\\ndiv.mdc-card {\\n  border-radius: 0px\\n}\\n\\n.mdc-card .card-header {\\n  display: flex;\\n}\\n\\n.mdc-card-title {\\n  font-family: \\\"Josefin Sans\\\";\\n  font-weight: bold;\\n  align-items: center;\\n  display: flex !important;\\n  position: relative !important;\\n}\\n\\n.mdc-card-title:nth-child(2) {\\n  margin-left: -1.4em;\\n}\\n\\n.pn-modal {\\n  overflow-y: scroll;\\n  width: 100%;\\n  display: none;\\n  position: absolute;\\n  top: 0;\\n  left: 0;\\n}\\n\\n.pn-modal-content {\\n  font-family: \\\"Josefin Sans\\\";\\n  background-color: #0e0e0e;\\n  margin: auto;\\n  margin-top: 25px;\\n  margin-bottom: 25px;\\n  padding: 15px 20px 20px 20px;\\n  border: 1px solid #888;\\n  width: 80% !important;\\n}\\n\\n.pn-modal-close {\\n  position: absolute;\\n  right: 25px;\\n  z-index: 100;\\n}\\n\\n.pn-modal-close:hover,\\n.pn-modal-close:focus {\\n  color: #000;\\n  text-decoration: none;\\n  cursor: pointer;\\n}\\n\\n.custom_button_bokeh button.bk-btn.bk-btn-default {\\n    font-size:48pt;\\n    background-color: #05b7ff;\\n    border-color: #05b7ff;\\n}\");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='cec8097f-ed81-4470-b3e4-6b0382eab1ca'>\n",
       "  <div id=\"e2b94548-cf9c-4924-89b8-fc343d4a2c67\" data-root-id=\"cec8097f-ed81-4470-b3e4-6b0382eab1ca\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"5fc46d5c-43b1-43b4-8bdc-23d4248f2fed\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"cec8097f-ed81-4470-b3e4-6b0382eab1ca\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"32b845e3-f3de-4091-8350-2cf8a52619b1\",\"attributes\":{\"plot_id\":\"cec8097f-ed81-4470-b3e4-6b0382eab1ca\",\"comm_id\":\"aadc4e869f4e4ce9be846929645b1707\",\"client_comm_id\":\"d0b40d69b77044c2961da7d5472e2262\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"5fc46d5c-43b1-43b4-8bdc-23d4248f2fed\",\"roots\":{\"cec8097f-ed81-4470-b3e4-6b0382eab1ca\":\"e2b94548-cf9c-4924-89b8-fc343d4a2c67\"},\"root_ids\":[\"cec8097f-ed81-4470-b3e4-6b0382eab1ca\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "cec8097f-ed81-4470-b3e4-6b0382eab1ca"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base imports\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cml.plot import initialize_bokeh\n",
    "from cml.panel import initialize_panel\n",
    "from jupyterthemes.stylefx import set_nb_theme\n",
    "from bokeh.io import show\n",
    "initialize_bokeh()\n",
    "initialize_panel()\n",
    "set_nb_theme(\"onedork\")\n",
    "rng = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"convolution\"></a>\n",
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In purely mathematical terms, convolution is a function derived from two given functions by integration which expresses how the shape of one is modified by the other. In simpler (discrete) terms, the convolution product of a matrix by a smaller one can be seen as _filtering_ the large matrix. Hence we slide the small matrix over the large one and compute local products at each position. Therefore the convolution operator $\\star$ computes at each position $n$\n",
    "\n",
    "$$\n",
    "(f \\star g)[n]=\\sum _{m=-M}^{M}f[n-m]g[m].\n",
    "$$\n",
    "\n",
    "An example of this operation is shown here\n",
    "\n",
    "<center>\n",
    "<img src=\"images/02_convolution.png\" align=\"center\"/>\n",
    "</center>\n",
    "\n",
    "This operation can be used to _filter_ the image (as in the _gaussian blur_ operator), or _detect_ features (such as edges). \n",
    "\n",
    "Given a 32x32 image with RGB channels, we can represent it as a tensor of shape `(3, 32, 32)` which is (channels, height, width). When we perform convolution, we need a filter that has the same channel depth as the image. For example, we can use a 5x5 filter which is of shape `(3, 5, 5)` and slide it across the image left to right, top to bottom with a stride of 1 to perform convolution. We are going to perform this in numpy, depending on a certain amount of parameters, which define the behavior of our convolution\n",
    "\n",
    "* `height` and `width`: spatial extend of the filters\n",
    "* `stride`: stride size (number of steps to jump to the next position)\n",
    "* `padding`: amount of padding (adding zeros in the original matrix)\n",
    "\n",
    "Here note that we define a simple _horizontal line detector_ kernel, but we will display other kernels that you can toy off with later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properties of our kernel\n",
    "pad = 2\n",
    "stride = 1\n",
    "height, width = 3, 3\n",
    "# Our convolution kernel\n",
    "weight = [\n",
    "[1, 1, 1],\n",
    "[1, 1.5, 1],\n",
    "[1, 1, 1],\n",
    "]\n",
    "weight = np.array(weight)\n",
    "# Final kernel (replicated over channels)\n",
    "kernel = np.repeat(weight[np.newaxis, :], 3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we need to import some image on which to apply our convolution operation. Here, we display a small code to retrieve an image directly from a URL. You can replace the desired image by the URL of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10745ab4060c4e2ab4013dc07428bb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'22e6c473-1ccd-4c1b-a90d-18caaf6e2e5f': {'version"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from cml.plot import image, center_plot, cml_figure\n",
    "# URL to load our example image from\n",
    "url = \"https://thumbnailer.mixcloud.com/unsafe/300x300/extaudio/6/b/a/b/d576-35c3-48a9-a01e-7dc4f0255e62.jpg\"\n",
    "response = requests.get(url)\n",
    "img = np.array(Image.open(BytesIO(response.content)))\n",
    "img = img.transpose(2, 0, 1)[:, :]\n",
    "plot = center_plot(image(img, title=\"Example image\"))\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_w, x_h = img.shape[1], img.shape[2]\n",
    "# Padding the original image\n",
    "x_pad = np.pad(img, pad_width=((0, 0,), (pad, pad), (pad, pad)), mode='constant', constant_values=0)\n",
    "# We can expect the output size to be\n",
    "h_out = int(1 + (x_h + 2 * pad - height) / stride)\n",
    "w_out = int(1 + (x_w + 2 * pad - width) / stride)\n",
    "# So we will store our result in\n",
    "y = np.zeros((1, h_out, w_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the convolution itself can be performed by using the following loop (which amounts to _slide_ our kernel across the large matrix). As we have seen in the course, it corresponds to computing the convolution equation\n",
    "\n",
    "$$\n",
    "(f \\star g)[n]=\\sum _{m=-M}^{M}f[n-m]g[m].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding the kernel across the matrix\n",
    "weight = [\n",
    "[-1, -2, -1],\n",
    "[0, 0, 0],\n",
    "[1, 2, 1],\n",
    "]\n",
    "for h in range(h_out):\n",
    "    for w in range(w_out):\n",
    "        i, j = h * stride, w * stride\n",
    "        conv_sum = np.sum(x_pad[:, i:i+height, j:j+width] * weight)\n",
    "        y[0, h, w] = conv_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can witness the effect of this operation with the following plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446e22a9020a4fcfa9a2265c2059b0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'c462e660-c9c2-4e14-a866-5748e7e19904': {'version"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bokeh.layouts import row\n",
    "plot = center_plot(row(image(img, title=\"Input\"), image(y, title=\"Convolved\")))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed previously, depending on the content (weights) of the kernel, different features are detected by different types of convolutional kernels, we outline here several kernels that you can play with (or even try your own) by modifying the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian blur\n",
    "weight = [\n",
    "[1, 1, 1],\n",
    "[1, 3, 1],\n",
    "[1, 1, 1],\n",
    "]\n",
    "# Sharpen\n",
    "weight = [\n",
    "[0, -1, 0],\n",
    "[-1, 5, -1],\n",
    "[0, -1, 0],\n",
    "]\n",
    "# Outline\n",
    "weight = [\n",
    "[-1, -1, -1],\n",
    "[-1, 8, -1],\n",
    "[-1, -1, -1],\n",
    "]\n",
    "# Detect vertical lines\n",
    "weight = [\n",
    "[-1, 0, 1],\n",
    "[-2, 0, 2],\n",
    "[-1, 0, 1],\n",
    "]\n",
    "# Detect horizontal lines\n",
    "weight = [\n",
    "[1, 2, 1],\n",
    "[0, 0, 0],\n",
    "[-1, -2, -1],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "Convolutional NNs (CNNs) rely on convolution in place of general matrix multiplication. They are specialized for processing data with a known grid-like topology and are among the best performing systems in classification/recognition tasks. Each layer in a CNN consists in a set of $N$ _filters_ called _kernels_, that are convolved across the input. If we denote as $\\{k^l_n\\}_{n\\in[1;N]}$ the set of kernels for layer $l$, these all share a unique _kernel size_. By convolving each one of its $N$ kernels across a d-dimensional input $x$, a convolutional layer produces $N$ d-dimensional outputs called _feature maps_, denoted as $\\{a^l_n\\}_{n\\in[1;N]}$. Hence, the computation of the $n$-th activation map in layer $l$ for input $x$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "a^l_m = \\sum_{n=1}^{N} k^l_m \\star x_n + b^l_m\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus, as depicted in the following Figure, the feature map corresponding to kernel $n$ consists in the sum of the d-dimensional discrete convolutions (denoted by the $\\star$ operator) between the kernel $n$ and each one of the d-dimensional data $\\{x_m\\}_{m\\in[1;M]}$, plus a bias $b$. A convolutional layer is thus a 3-dimensional tensor $h \\in \\mathcal{T}_{N,I,J}(\\mathbb{R})$ where $N$ is the number of features maps while $I$ and $J$ are respectively the _width_ and _height_ of the maps. \n",
    "\n",
    "<img src=\"images/02_cnns.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be seen as replacing our _neurons_ by _feature detectors_ (the convolutional kernels), which will increasingly process the image. In the following, we will first use the high-level interface of `Pytorch` to define a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a CNN in `JAX`\n",
    "\n",
    "First, we are going to start by implementing the whole network in pure `JAX`. This will allow us to get a sense of how low-level operations can impact the overall design in the learning steps. We start by importing the required libraries (`JAX`, `Optax`, and their related modules). As we are on an historic path, we will be using the [`MNIST` dataset](https://en.wikipedia.org/wiki/MNIST_database), which is a digit classification dataset used for a long time as a reference in machine learning to test models (this dataset is still widely used in order to perform benchmarking on low-level properties of a network or to perform debugging).\n",
    "\n",
    "### Quick note on `JAX` vs. `Pytorch`\n",
    "\n",
    "In `JAX`, parameters are typically represented as plain numpy arrays or `JAX` DeviceArrays, and transformations on these arrays are expressed as **pure functions**. This approach is called **\"functional programming\"** and is a key feature of `JAX`. One advantage of this approach is that it allows for more explicit control over the state of the model. Since parameters are represented as plain arrays, it is easy to reason about how the state of the model changes over time. Additionally, it can be easier to implement certain optimizations (such as gradient checkpointing) using a functional programming approach.\n",
    "\n",
    "In `PyTorch`, parameters are typically represented as object-oriented modules, which encapsulate both the parameters themselves and the transformations that act on them. This approach is called **\"imperative programming\"** and is a key feature of `PyTorch`. One advantage of the object-oriented approach is that it allows for more flexible and modular code organization. Modules can be easily combined to create complex models, and the encapsulation of parameters and transformations within modules can make the code more readable and easier to reason about.\n",
    "\n",
    "Overall, both approaches have their advantages and disadvantages, and the choice between them may depend on the specific use case and personal preference. However, it is worth noting that `JAX` is generally better suited for large-scale machine learning applications, while `PyTorch` is often preferred for rapid prototyping and experimentation. Note also that several libraries exist for `JAX` that emulates this object-oriented behavior, and we will also be relying on those. Here are a few examples\n",
    "- [Flax](https://flax.readthedocs.io/en/latest/) provides a high-level API for defining neural networks and includes a variety of pre-built models and utilities.\n",
    "- [Stax](https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.stax.html) provides a functional API for defining neural network architectures with pre-built layers. \n",
    "- [Haiku](https://dm-haiku.readthedocs.io/en/latest/) also provides a high-level API for networks using a functional programming style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset: \n",
    "\n",
    "Next, we are going to load the MNIST dataset and preprocess it by reshaping the input data to be in the format (batch_size, height, width, channels), normalizing the pixel values by dividing by 255, and converting the labels to integer format. Note here that we show how we can get the `MNIST` dataset from `TensorFlow` (in the `Keras` library), but throughout the course, we will be using different sources of data so that you can use any you like in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, grad\n",
    "import optax\n",
    "from tensorflow.keras.datasets import mnist\n",
    "# Obtain the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# Perform [0, 1] normalization\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype(jnp.float32) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype(jnp.float32) / 255.0\n",
    "y_train = y_train.astype(jnp.int32)\n",
    "y_test = y_test.astype(jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[4])\n",
    "n_example = 1000\n",
    "X_train.reshape(n_example, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNN architecture\n",
    "\n",
    "Here, we are going to implement the CNN architecture using pure JAX functions. In this example, we use two convolutional layers, each followed by a ReLU activation. **Note that the gradient of the _pooling_ operation being slightly peculiar, we leave it for the following exercise**. Instead, we will rely on _strided convolutions_ to still obtain some dimensionality reduction across the different layers. Afterward, we flatten the output and pass it through a dense layer for classification.\n",
    "\n",
    "**We provide the full code to make this model works, but we strongly encourage you to try to recode the whole network first without looking at the actual solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercise (course)\n",
    ">   1. Implement the convolution, pooling operation in JAX\n",
    ">   2. Define your CNN based on those defined operations\n",
    ">   3. Train your model on the MNIST dataset\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Implementing the operations\n",
    "\n",
    "Here you first need to define how to perform a convolution operation, which handles the padding and also how to define a linear (dense) layer. You can look into the following JAX operations.\n",
    "- `jax.lax.pad`: This operation pads an array with zeros along one or more dimensions. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.pad.html)\n",
    "- `jax.lax.conv_general_dilated`: This operation computes a n-dimensional convolution with dilated filter. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.conv_general_dilated.html)\n",
    "- `jax.nn.relu`: This operation applies the Rectified Linear Unit (ReLU) function element-wise to an input array. [Documentation](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.relu.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE - JAX Operations\n",
    "######################\n",
    "def conv2d(x, W, cb):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "# Solution: \n",
    "def conv2d(x, W, b, stride=2):\n",
    "    padding_height = max((x.shape[1] - 1) * stride + W.shape[0] - x.shape[1], 0)\n",
    "    padding_width = max((x.shape[2] - 1) * stride + W.shape[1] - x.shape[2], 0)\n",
    "    pad_top = padding_height // 2\n",
    "    pad_bottom = padding_height - pad_top\n",
    "    pad_left = padding_width // 2\n",
    "    pad_right = padding_width - pad_left\n",
    "    padded_x = jax.lax.pad(x, 0.0, ((0, 0, 0), (pad_top, pad_bottom, 0), (pad_left, pad_right, 0), (0, 0, 0)))\n",
    "    return jax.lax.conv_general_dilated(padded_x, W, (stride, stride), ((1, 1), (1, 1)), dimension_numbers=('NHWC', 'HWIO', 'NHWC')) + b\n",
    "\n",
    "def dense(x, W, b):\n",
    "    return jnp.dot(x, W) + b\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Defining the CNN model\n",
    "\n",
    "Then, defining the CNN model should be a quite simple succession of your defined operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE - CNN Model\n",
    "######################\n",
    "def cnn(x, params):\n",
    "    W1, W2, K1 = params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################\n",
    "# Solution: \n",
    "def cnn(params, x):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = params\n",
    "    # First layer\n",
    "    x = conv2d(x, W1, b1)\n",
    "    x = jax.nn.relu(x)\n",
    "    # Second layer\n",
    "    x = conv2d(x, W2, b2)\n",
    "    x = jax.nn.relu(x)\n",
    "    # Third layer\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = dense(x, W3, b3)\n",
    "    x = jax.nn.relu(x)\n",
    "    # Final (MLP) layer\n",
    "    x = dense(x, W4, b4)\n",
    "    return x\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Define the loss function\n",
    "\n",
    "Implement a loss function that computes the categorical cross-entropy loss between the predicted logits and the true labels. To do so, you can rely on the following JAX functions\n",
    "- `jax.nn.one_hot`: This operation converts integer labels to a one-hot representation. [Documentation:](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.one_hot.html)\n",
    "- `optax.softmax_cross_entropy`: This operation computes the cross-entropy loss between a set of predicted probabilities (using softmax) and a set of true labels. [Documentation](https://optax.readthedocs.io/en/latest/api.html#optax.softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE - CNN Model\n",
    "######################\n",
    "\n",
    "######################\n",
    "# Solution: \n",
    "\n",
    "def loss(params, x, y):\n",
    "    logits = cnn(params, x)\n",
    "    return jnp.mean(optax.softmax_cross_entropy(logits, jax.nn.one_hot(y, 10)))\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Initializing the weights and optimizer\n",
    "\n",
    "Here, we implement a loss function that computes the categorical cross-entropy loss between the predicted logits and the true labels. Then, we set up the Adam optimizer using Optax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(rng):\n",
    "    # Initialize parameters\n",
    "    rng, *keys = random.split(rng, 5)\n",
    "    W1 = random.normal(keys[0], (3, 3, 1, 32)) * jnp.sqrt(2 / (3 * 3 * 1))\n",
    "    b1 = jnp.zeros(32)\n",
    "    W2 = random.normal(keys[1], (3, 3, 32, 64)) * jnp.sqrt(2 / (3 * 3 * 32))\n",
    "    b2 = jnp.zeros(64)\n",
    "    W3 = random.normal(keys[2], (57600, 128)) * jnp.sqrt(2 / (7 * 7 * 64))\n",
    "    b3 = jnp.zeros(128)\n",
    "    W4 = random.normal(keys[3], (128, 10)) * jnp.sqrt(2 / 128)\n",
    "    b4 = jnp.zeros(10)\n",
    "    return W1, b1, W2, b2, W3, b3, W4, b4\n",
    "# Prepare random generator\n",
    "rng = random.PRNGKey(42)\n",
    "# Obtain the dictionary of parameters\n",
    "params = init_params(rng)\n",
    "# Create our optimizer\n",
    "optimizer = optax.adam(1e-3)\n",
    "# Initialize the optimization state\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we provide here the code for the main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m X_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m     15\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m y_train[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m---> 16\u001b[0m params, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def update(params, opt_state, x, y):\n",
    "    grads = jax.grad(loss)(params, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_x = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "        params, opt_state = update(params, opt_state, batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our network\n",
    "\n",
    "In the following, we provide a simplistic code for evaluating the results of our network. We take a random batch from the test set and pass it through the network to obtain its infered label and display the results by printing the infered label above each given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n",
      "findfont: Generic family 'sans-serif' not found because none of the following families were found: Helvetica\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x10000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cml.plot import cml_figure_matplotlib\n",
    "test_size = 4\n",
    "# Pick a random batch of input images\n",
    "start_idx = np.random.randint(0, X_test.shape[0] - test_size)\n",
    "x_batch = X_test[start_idx:start_idx+test_size]\n",
    "# Perform inference on test data\n",
    "predicted_probabilities = cnn(params, x_batch)\n",
    "# Get predicted classes\n",
    "predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "# Define class names\n",
    "class_names = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "# Get corresponding class names\n",
    "class_labels = [class_names[i] for i in predicted_classes]\n",
    "# Plot results\n",
    "fig = cml_figure_matplotlib(100, 20); \n",
    "axs = fig.subplots(1, 4)\n",
    "for i in range(test_size):\n",
    "    axs[i].imshow(x_batch[i]); axs[i].set_title(class_labels[i]);\n",
    "    axs[i].set_xticks([]); axs[i].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cnn\"></a>\n",
    "## Exercise 1 - Defining a CNN in Pytorch\n",
    "\n",
    "Defining a convolutional network in Pytorch is quite easy, as we can rely on the `nn` module, which contains all the required layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous course, we have seen that we could define our network in a very simple way, by using the `Sequential` model definition. Here we define a CNN followed by a MLP, as seen in the previous course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the input dimensions\n",
    "in_size = 1000\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(6, 16, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "inputs = torch.rand(16, 3, 32, 32)\n",
    "model(inputs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement the networks in Pytorch is to use the `functional` approach. In this version, each layer is seen as a function, that we apply on sucessive inputs. For instance, we can define one layer of fully-connected units and apply it to some inputs as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Define one layer\n",
    "layer = nn.Linear(100, 10)\n",
    "# Define the non-linearity\n",
    "activation = nn.Sigmoid()\n",
    "# Create some random input\n",
    "inputs = torch.rand(32, 100)\n",
    "# inputs.view(16, -1) => 200\n",
    "# inputs.view(16, -1, 4) => 50\n",
    "# Apply our layers\n",
    "outputs = activation(layer(inputs))\n",
    "# Equivalently, as ReLU is parameter-free\n",
    "outputs = F.relu(layer(inputs))\n",
    "y = (outputs + 3).sum()\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make this even cleaner, we can define our own `nn.Module`, which is a `Pytorch` class representing models. To do so, we can define a sub-class, and implement the functions `__init__` (defining our layers) and `forward` (explaining how our forward pass will behave)\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.1 - Implement a CNN by **using the `functional` library**\n",
    "\n",
    "> 1. Update the forward propagation and error computation\n",
    "> 2. Update the back-propagation part to learn the weights of both layers.\n",
    "  \n",
    "*For optional questions, please look at the end of this exercise's code boxes for more information*\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "In order to test our CNN, we are going to try to perform image classification. To do so, we can use the simplifications for data loading contained in `torchvision`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torchvision` package provides pre-coded simplification to download and use the major image datasets, notably `MNIST` and `CIFAR`, which are the baseline datasets for testing image ML models. The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "In the following code, we are going to load the `CIFAR10` _train_ and _test_ sets. **Note that this code will automatically download the dataset if you did not have it before, and place it in the `data` folder, so this might take a bit of time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transforms to apply to the images\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Import the train dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "# Import the test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "# Classes in the CIFAR dataset\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your knowledge from the previous course, you can now define an optimization problem, and implement the training loop for your model\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 1.2 - Training our `functional` CNN\n",
    "\n",
    "> 1. Define a `criterion` and `optimizer`\n",
    "> 2. Fill in the training loop to train your model\n",
    "  \n",
    "*For optional questions, please look at the end of this exercise's code boxes for more information*\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #410819; border-color: #cb2e47\">\n",
    "\n",
    "> **Warning on the training time**\n",
    "\n",
    "> The CIFAR dataset that we are using is a very large one, and given that we are using a deep architecture, the full training of the model will take a prohibitive time on a CPU. We recommend that you either ensure that you use the GPU of your own computer or (if unavailable) try out your code on the Colab version with a GPU runtime.\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.219\n",
      "[1,  4000] loss: 1.859\n",
      "[1,  6000] loss: 1.654\n",
      "[1,  8000] loss: 1.555\n",
      "[1, 10000] loss: 1.512\n",
      "[1, 12000] loss: 1.449\n",
      "[2,  2000] loss: 1.391\n",
      "[2,  4000] loss: 1.373\n",
      "[2,  6000] loss: 1.321\n",
      "[2,  8000] loss: 1.295\n",
      "[2, 10000] loss: 1.273\n",
      "[2, 12000] loss: 1.275\n",
      "[3,  2000] loss: 1.201\n",
      "[3,  4000] loss: 1.202\n",
      "[3,  6000] loss: 1.189\n",
      "[3,  8000] loss: 1.161\n",
      "[3, 10000] loss: 1.193\n",
      "[3, 12000] loss: 1.163\n",
      "[4,  2000] loss: 1.077\n",
      "[4,  4000] loss: 1.105\n",
      "[4,  6000] loss: 1.112\n",
      "[4,  8000] loss: 1.102\n",
      "[4, 10000] loss: 1.092\n",
      "[4, 12000] loss: 1.101\n",
      "[5,  2000] loss: 0.996\n",
      "[5,  4000] loss: 1.024\n",
      "[5,  6000] loss: 1.050\n",
      "[5,  8000] loss: 1.045\n",
      "[5, 10000] loss: 1.033\n",
      "[5, 12000] loss: 1.027\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "n_epochs = 5\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your model is trained, you can test it by feeding some new (unseen) images and see if it is able to classify them correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"layer\"></a>\n",
    "## Coding our own convolutional layer\n",
    "\n",
    "Although `Pytorch` comes packed with pre-implemented layers, we can also very easily define our own layers. This will be useful when you start doing research and propose your own way of processing the information. A large advantage of `Pytorch` is that it performs _automatic gradient differentiation_, this means that we simply have to define how the `forward` pass will work, and `Pytorch` will automatically infer the backpropagation equations, without us having to go through any complicated differentiation\n",
    "\n",
    "In the following, we are going to redefine the `Conv2d` layer, by computing the operation ourselves.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Going further (optional)\n",
    ">   1. Complete the `forward` function to compute a convolution\n",
    ">   2. Do the same for the pooling operation\n",
    ">   3. Rewrite your previous Pytorch CNN using your own modules\n",
    ">   2. Train this model on the MNIST dataset\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyConv2d(nn.Module):\n",
    "    def __init__(self, n_channels, out_channels, kernel_size, dilation=1, padding=0, stride=1):\n",
    "        super(MyConv2d, self).__init__()\n",
    "        self.kernel_size = (kernel_size, kernel_size)\n",
    "        self.kernal_size_number = kernel_size * kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = (dilation, dilation)\n",
    "        self.padding = (padding, padding)\n",
    "        self.stride = (stride, stride)\n",
    "        self.n_channels = n_channels\n",
    "        self.weights = nn.Parameter(torch.Tensor(self.out_channels, self.n_channels, self.kernal_size_number))\n",
    "\n",
    "    def forward(self, x):\n",
    "        width = self.compute_width(x)\n",
    "        height = self.compute_height(x)\n",
    "        windows = self.compute_windows(x)\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "\n",
    "        return x  \n",
    "\n",
    "    def compute_windows(self, x):\n",
    "        windows = F.unfold(x, kernel_size=self.kernel_size, padding=self.padding, dilation=self.dilation, stride=self.stride)\n",
    "        windows = windows.transpose(1, 2).contiguous().view(-1, x.shape[1], self.kernal_size_number)\n",
    "        windows = windows.transpose(0, 1)\n",
    "        return windows\n",
    "\n",
    "    def compute_width(self, x):\n",
    "        return ((x.shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0]) + 1\n",
    "\n",
    "    def compute_height(self, x):\n",
    "        return ((x.shape[3] + 2 * self.padding[1] - self.dilation[1] * (self.kernel_size[1] - 1) - 1)// self.stride[1]) + 1\n",
    "\n",
    "# Testing the code directly\n",
    "conv = MyConv2d(3, 1, 3)\n",
    "x = torch.randn(1, 3, 24, 24)\n",
    "out = conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use your own `MyConv2d` layer, and use it in real-life scenarios. Hence, you need to first implement the pooling operation and then trying to change your previous model to use your own layer instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Recurrent Neural Networks (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are a family of models designed to process time series and sequential data, which perform remarkably in applications such as speech recognition or machine translation. The ability of RNNs to model correlations between successive computations through recurrent connection make them efficient for temporal problem as they provide a form of _memory_. \n",
    "\n",
    "To model structured sequential data, NNs can be augmented with recurrent loops, which allow to retain information across time steps. Considering a sequence $\\mathbf{X}=\\{\\mathbf{x}_t\\}$, dependencies between elements are managed by having a recurrent hidden state $\\mathbf{h}_t$ at time $t$ in the network. The value of $\\mathbf{h}_t$ at each time depends of the previous time and the input, as depicted in the following figure. \n",
    "\n",
    "<img src=\"images/02_rnn.png\" align=\"center\"/>\n",
    "\n",
    "Formally, each hidden state is updated as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{h}_t = \n",
    "\\begin{cases} \n",
    "\\phi_{\\mathbf{\\mathbf{\\theta}}}(\\mathbf{x}_0) & \\mbox{if } t=0 \\\\ \n",
    "\\phi_{\\mathbf{\\mathbf{\\theta}}}(\\mathbf{h}_{t-1},\\mathbf{x}_t), & \\mbox{otherwise} \n",
    "\\end{cases}\n",
    "\\label{eq:RNNhiddenupdate}\n",
    "\\end{equation}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple one-to-many vanilla recurrent neural network example in functional form. If we were to produce `h[t]`, we need some weight matrices, `h[t-1]`, `x[t]` and a non-linearity `tanh`.\n",
    "\n",
    "$$\n",
    "h_{t} = tanh(W_{hh}h_{t-1} + W_{xh}x_{t} + B_{h})\n",
    "$$\n",
    "\n",
    "Since this is a **one-to-many** network, we'd want to produce an output `y[t]` at every timestep, thus, we need another weight matrix that accepts a hidden state and project it to an output.\n",
    "\n",
    "$$\n",
    "y_{t} = W_{hy}h_{t} + B_{y}\n",
    "$$\n",
    "\n",
    "Now that we know how to use the `Functional` library of `Pytorch`, we are going to implement our own simple RNN layer as previously. This time, we do not provide the content of the `__init__` function, so think carefully of what parameters you will need and how you need to define them.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #013220; border-color: #03C03C\">\n",
    "\n",
    "> ### Question 2.1 - Defining a `functional` RNN\n",
    "\n",
    "> 1. Complete the `__init__` function to instantiate all required parameters\n",
    "> 2. Complete the `forward` function to compute the forward pass\n",
    "\n",
    "  \n",
    "*For optional questions, please look at the end of this exercise's code boxes for more information*\n",
    "\n",
    "</div>\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecurrentNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\" Defines the required variables \"\"\"\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.Wxh = nn.Linear(input_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Why = nn.Linear(hidden_size, output_size)\n",
    "        self.state = torch.zeros(hidden_size)\n",
    "\n",
    "\n",
    "    def init_state(self):\n",
    "        self.state = torch.zeros(self.hidden_size)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = torch.zeros(self.hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Computes the forward pass \"\"\"\n",
    "        input = x\n",
    "        state = self.state\n",
    "        self.init_state()\n",
    "        self.state = torch.tanh(self.Wxh(input) + self.Whh(state))\n",
    "        output = self.Why(self.state)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then forward propagate our information inside our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839],\n",
      "        [ 0.2728, -0.1385, -0.6281,  0.2849,  0.2839]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434],\n",
      "        [ 0.1390,  0.0228, -0.4879,  0.3197,  0.2434]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552],\n",
      "        [ 0.0352, -0.0391, -0.4908,  0.2994,  0.3552]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_size = 4\n",
    "input_vector = torch.ones(16, in_size)\n",
    "simple_network = RecurrentNetwork(in_size, 10, 5)\n",
    "# Notice that same input, but leads to different ouptut at every single time step.\n",
    "print(simple_network(input_vector))\n",
    "print(simple_network(input_vector))\n",
    "print(simple_network(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, `Pytorch` also comes packed with some pre-coded recurrent layers. You can go check the documentation to find how to use these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music generation with LSTMs \n",
    "\n",
    "In this exercise, we will explore how to use LSTMs (Long Short-Term Memory) for symbolic music generation with the help of `PyTorch`. Music generation with LSTM is a challenging problem where the goal is to train a model to generate new pieces of music that are musically coherent and similar to a given input dataset. However, there are several challenges associated with this task, such as capturing the long-term dependencies and structures that exist in music, dealing with the high dimensionality and complexity of musical data, and balancing the need for creativity and novelty with the desire for consistency and coherence. In addition, there is the issue of evaluating the quality of generated music, which can be subjective and context-dependent. Despite these challenges, recent advancements in deep learning techniques and models such as LSTMs have shown promising results in generating high-quality music, and the field continues to be an active area of research and development.\n",
    "\n",
    "We will provide here the blueprint for adressing this problem by using a \"mid-level\" approach. Compared to the previous problems, we will heavily rely on the pre-defined layers available in `Pytorch` and `JAX` (through the `Flax` library). However, we will still need to define the overall approach to solve this problem, notably for the data pre-processing, model and loss definition and the overall training loop. The major library we will be using is the `torch.nn` as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Usually, symbolic music scores in computers come in the [MIDI format](https://en.wikipedia.org/wiki/MIDI). MIDI files contain information about the _pitch_, _timing_, and _duration_ of musical notes, as well as other metadata such as tempo and instrument type. However, these are events that only provide information at various time positions. Hence, to train an LSTM model, the MIDI data needs to be converted into a suitable format that can be used as input to the model. This involves several preprocessing steps, such as **quantization**, **encoding**, and **normalization**. \n",
    "- _Quantization_ involves dividing the MIDI data into fixed time intervals and mapping each note to the nearest quantized time step. \n",
    "- _Encoding_ involves converting the quantized MIDI data into a numerical representation, such as a one-hot encoding or a continuous representation using embeddings. \n",
    "- _Normalization_ involves scaling the encoded data to a fixed range to ensure that the LSTM can learn meaningful patterns and avoid saturation or vanishing gradients. \n",
    "\n",
    "Overall, preprocessing the MIDI data is a crucial step in preparing the data for deep learning with LSTMs and can have a significant impact on the quality of the generated music. However, as this is a quite complicated set of operations, we will rather rely on a dataset that has already been preprocessed and is one of the \"golden standard\" in symbolic music generation, the [JSB Chorales dataset](https://github.com/czhuang/JSB-Chorales-dataset)\n",
    "\n",
    "**Note for other input data**\n",
    "If you want to try out your network on other data, and learn how to implement your own data pre-processing methods, we recommend trying out the [Groove dataset](https://www.tensorflow.org/datasets/catalog/groove) from TFDS, which contains a large number of drum performances (as scores) in [MIDI format](https://en.wikipedia.org/wiki/MIDI).\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "Thanks to the great work on the [JSB Chorales github](https://github.com/czhuang/JSB-Chorales-dataset), we just need to retrieve a pickled (zipped) array and import it to obtain a prepared dataset for music generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from cml.plot import image, center_plot, cml_figure\n",
    "# URL to load the JSB chorales dataset from\n",
    "url = \"https://github.com/czhuang/JSB-Chorales-dataset/raw/master/jsb-chorales-8th.pkl\"\n",
    "response = requests.get(url)\n",
    "with open(\"./data/jsb_chorales.pkl\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "with open(\"./data/jsb_chorales.pkl\", \"rb\") as p:\n",
    "    data = pickle.load(p, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing examples from the training set\n",
    "\n",
    "A very important step before designing a model is to carefully scrutinize the properties of the data corresponding to the task we wish to address.\n",
    "\n",
    "Here, each training example in the JSB Chorales dataset contains a sequence of 16th notes represented as integers, where each integer corresponds to a MIDI note number or a rest. The notes are represented using a simplified version where a note is represented as an integer in the range [0, 127], and a rest is represented as -1. The sequences represent the soprano, alto, tenor, and bass voices in four-part chorales from the J.S. Bach chorale harmonization collection. The chorales are all in 4/4 time, and each example consists of exactly 64 beats (i.e., 256 16th notes). \n",
    "\n",
    "The dataset contains a total of 229 chorales, split into 182 for training, 21 for validation, and 26 for testing. The dataset was created to facilitate research into automatic music composition and machine learning for music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, 70, 65, 58), (74, 70, 65, 58), (75, 70, 58, 55), (75, 70, 60, 55), (77, 69, 62, 50), (77, 69, 62, 50), (77, 70, 62, 55), (77, 69, 62, 55), (75, 67, 63, 48), (75, 69, 63, 48), (74, 70, 65, 46), (74, 70, 65, 46), (72, 69, 65, 53), (72, 69, 65, 53), (72, 69, 65, 53), (72, 69, 65, 53), (74, 70, 65, 46), (74, 70, 65, 46), (75, 69, 63, 48), (75, 67, 63, 48), (77, 65, 62, 50), (77, 65, 60, 50), (74, 67, 58, 55), (74, 67, 58, 53), (72, 67, 58, 51), (72, 67, 58, 51), (72, 65, 57, 53), (72, 65, 57, 53), (70, 65, 62, 46), (70, 65, 62, 46), (70, 65, 62, 46), (70, 65, 62, 46), (72, 69, 65, 53), (72, 69, 65, 53), (74, 71, 53, 50), (74, 71, 53, 50), (75, 72, 55, 48), (75, 72, 55, 50), (75, 67, 60, 51), (75, 67, 60, 53), (74, 67, 60, 55), (74, 67, 57, 55), (74, 65, 59, 43), (72, 63, 59, 43), (72, 63, 55, 48), (72, 63, 55, 48), (72, 63, 55, 48), (72, 63, 55, 48), (75, 67, 60, 60), (75, 67, 60, 60), (77, 70, 62, 58), (77, 70, 62, 56), (79, 70, 62, 55), (79, 70, 62, 53), (79, 70, 63, 51), (79, 70, 63, 51), (77, 70, 63, 58), (77, 70, 60, 58), (77, 70, 62, 46), (77, 68, 62, 46), (75, 67, 58, 51), (75, 67, 58, 51), (75, 67, 58, 51), (75, 67, 58, 51), (74, 67, 58, 55), (74, 67, 58, 55), (75, 67, 58, 53), (75, 67, 58, 51), (77, 65, 58, 50), (77, 65, 56, 50), (70, 63, 55, 51), (70, 63, 55, 51), (75, 65, 60, 45), (75, 65, 60, 45), (74, 65, 58, 46), (74, 65, 58, 46), (72, 65, 57, 53), (72, 65, 57, 53), (72, 65, 57, 53), (72, 65, 57, 53), (74, 65, 58, 58), (74, 65, 58, 58), (75, 67, 58, 57), (75, 67, 58, 55), (77, 65, 60, 57), (77, 65, 60, 53), (74, 65, 58, 58), (74, 65, 58, 58), (72, 67, 58, 51), (72, 67, 58, 51), (72, 65, 57, 53), (72, 65, 57, 53), (70, 65, 62, 46), (70, 65, 62, 46), (70, 65, 62, 46), (70, 65, 62, 46)]\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, based on the observed data, we can start thinking on how we could design an adequate model for processing this data. As we know that this form of data is clearly temporal, we will rely on network composed of LSTM cells. We can cast this task as an _inference_ problem, in which we try to **predict from a given sequence of notes the most probable next note**. Note that this allows us to straightforwardly use our trained network to generate entirely novel pieces of music, we just need to give different starting sequences, and then predict each note one by one (we feed the network with its own results from the past steps).\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-success\" markdown=\"1\" style=\"color:white; background-color: #192841; border-color: #779ecb\">\n",
    "\n",
    "> ### Exercice (optional)\n",
    ">   1. Create a model that can train on the proposed input data from JSB Chorales\n",
    ">   2. Use at least one LSTM layer and an output layer for _prediction_\n",
    ">   3. Write the training function for your model\n",
    ">   4. Complete the training loop and launch your model\n",
    "\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "\n",
    "In this step, you will implement an RNN or LSTM model using PyTorch. Design the architecture of your choice, but make sure it has at least one LSTM layer and an output layer with a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(MusicRNN, self).__init__()\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Now that our model is defined, we can train it on our preprocessed music data. Implement the training function using the appropriate loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "   \n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can define the final training loop for your music generation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, dataloader, optimizer and loss function\n",
    "input_size = 1\n",
    "hidden_size = 512\n",
    "output_size = len(note_to_int)\n",
    "num_layers = 1\n",
    "batch_size = 64\n",
    "# Try to use a GPU if we have one available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "# Create our model\n",
    "model = ...\n",
    "# Create our optimizer and criterion\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "n_steps = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    ######################\n",
    "    # YOUR CODE GOES HERE\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Generation\n",
    "With our trained model, we can now generate new music sequences. Implement a function that takes an initial sequence as input and generates a new sequence of notes and chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, initial_sequence, length, device):\n",
    "    model.eval()\n",
    "    generated_sequence = []\n",
    "\n",
    "    for _ in range(length):\n",
    "    \n",
    "        ######################\n",
    "        # YOUR CODE GOES HERE\n",
    "        ######################\n",
    "        pass\n",
    "\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listening to the results\n",
    "\n",
    "In order to be able to listen to the beautiful results of your music generation LSTM, we provide here a code that is able to synthesize audio from a MIDI file so that you can directly hear your results :)\n",
    "\n",
    "**Note that we apply this to an example from the training data, you need to change the following code so that it uses your `generate_music` function instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stream\n",
      "Opening player\n",
      "Launching play (may take a while)\n"
     ]
    }
   ],
   "source": [
    "import music21\n",
    "import random\n",
    "# Select a random training example\n",
    "example = random.choice(data['train'])\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "# Convert the example notes to a music21 stream\n",
    "stream = music21.stream.Stream()\n",
    "print(\"Preparing stream\")\n",
    "for i, voice in enumerate(('soprano', 'alto', 'tenor', 'bass')):\n",
    "    part = music21.stream.Part(id=voice)\n",
    "    for note_f in example:\n",
    "        note = note_f[i]\n",
    "        if note == -1:  # Rest\n",
    "            n = music21.note.Rest()\n",
    "        else:  # Note\n",
    "            n = music21.note.Note(note)\n",
    "        part.append(n)\n",
    "    stream.append(part)\n",
    "# Play the music21 stream\n",
    "print(\"Opening player\")\n",
    "sp = music21.midi.realtime.StreamPlayer(stream)\n",
    "# Launch playing\n",
    "print(\"Launching play (may take a while)\")\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
